{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4027a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ROOT = '/Users/wuqianran/Desktop/bigdata_finalproject/final'\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f252a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n",
    "                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "        return df\n",
    "\n",
    "    def filter_cols(df):\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "                if isnull > 0.95:\n",
    "                    df = df.drop(col)\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "class Aggregator:\n",
    "    # Please add or subtract features yourself, be aware that too many features will take up too much space.\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]\n",
    "        expr_var = [pl.var(col).alias(f\"var_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max  + expr_mean \n",
    "\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max  + expr_mean \n",
    "\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        # expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        return expr_max  + expr_mean\n",
    "    \n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        return expr_max  + expr_mean\n",
    "\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        return expr_max + expr_mean\n",
    "\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "\n",
    "        return exprs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d92da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    if depth in [1,2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n",
    "    return df\n",
    "\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    \n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "    \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c3674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base\n",
    "\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f75714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21980cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 1min 27s, total: 3min 30s\n",
      "Wall time: 57.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ROOT            = Path(ROOT)\n",
    "\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648fdd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:\t (1526659, 720)\n",
      "CPU times: user 12.7 s, sys: 7.22 s, total: 19.9 s\n",
      "Wall time: 11.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_train = feature_eng(**data_store)\n",
    "print(\"train data shape:\\t\", df_train.shape)\n",
    "del data_store\n",
    "df_train = df_train.pipe(Pipeline.filter_cols)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b7b547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in ./new_venv/lib/python3.9/site-packages (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c08fabf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hz/nzpq1tbx5hg7f9n6msnh8mg40000gn/T/ipykernel_79098/537098600.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train data shape:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hz/nzpq1tbx5hg7f9n6msnh8mg40000gn/T/ipykernel_79098/757943805.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(df_data, cat_cols)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcat_cols\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcat_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/bigdata_finalproject/final/new_venv/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_pandas'"
     ]
    }
   ],
   "source": [
    "df_train, cat_cols = to_pandas(df_train)\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "print(\"train data shape:\\t\", df_train.shape)\n",
    "nums=df_train.select_dtypes(exclude='category').columns\n",
    "from itertools import combinations, permutations\n",
    "#df_train=df_train[nums]\n",
    "nans_df = df_train[nums].isna()\n",
    "nans_groups={}\n",
    "for col in nums:\n",
    "    cur_group = nans_df[col].sum()\n",
    "    try:\n",
    "        nans_groups[cur_group].append(col)\n",
    "    except:\n",
    "        nans_groups[cur_group]=[col]\n",
    "del nans_df; x=gc.collect()\n",
    "\n",
    "def reduce_group(grps):\n",
    "    use = []\n",
    "    for g in grps:\n",
    "        mx = 0; vx = g[0]\n",
    "        for gg in g:\n",
    "            n = df_train[gg].nunique()\n",
    "            if n>mx:\n",
    "                mx = n\n",
    "                vx = gg\n",
    "            #print(str(gg)+'-'+str(n),', ',end='')\n",
    "        use.append(vx)\n",
    "        #print()\n",
    "    print('Use these',use)\n",
    "    return use\n",
    "\n",
    "def group_columns_by_correlation(matrix, threshold=0.8):\n",
    "    # 计算列之间的相关性\n",
    "    correlation_matrix = matrix.corr()\n",
    "\n",
    "    # 分组列\n",
    "    groups = []\n",
    "    remaining_cols = list(matrix.columns)\n",
    "    while remaining_cols:\n",
    "        col = remaining_cols.pop(0)\n",
    "        group = [col]\n",
    "        correlated_cols = [col]\n",
    "        for c in remaining_cols:\n",
    "            if correlation_matrix.loc[col, c] >= threshold:\n",
    "                group.append(c)\n",
    "                correlated_cols.append(c)\n",
    "        groups.append(group)\n",
    "        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n",
    "    \n",
    "    return groups\n",
    "\n",
    "uses=[]\n",
    "for k,v in nans_groups.items():\n",
    "    if len(v)>1:\n",
    "            Vs = nans_groups[k]\n",
    "            #cross_features=list(combinations(Vs, 2))\n",
    "            #make_corr(Vs)\n",
    "            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n",
    "            use=reduce_group(grps)\n",
    "            uses=uses+use\n",
    "            #make_corr(use)\n",
    "    else:\n",
    "        uses=uses+v\n",
    "    print('####### NAN count =',k)\n",
    "print(uses)\n",
    "print(len(uses))\n",
    "uses=uses+list(df_train.select_dtypes(include='category').columns)\n",
    "print(len(uses))\n",
    "df_train=df_train[uses]\n",
    "# df_train.drop(['requesttype_4525192L_cnt','max_empl_employedtotal_800L_cnt', 'max_empl_industry_691L_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2446ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train[\"target\"]\n",
    "weeks = df_train[\"WEEK_NUM\"]\n",
    "df_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4262e332",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m best_auc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     26\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx_train, idx_valid \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcv\u001b[49m\u001b[38;5;241m.\u001b[39msplit(df_train, y, groups\u001b[38;5;241m=\u001b[39mweeks):\u001b[38;5;66;03m#   Because it takes a long time to divide the data set, \u001b[39;00m\n\u001b[1;32m     29\u001b[0m     X_train, y_train \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39miloc[idx_train], y\u001b[38;5;241m.\u001b[39miloc[idx_train]\u001b[38;5;66;03m# each time the data set is divided, two models are trained to each other twice, which saves time.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     X_valid, y_valid \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39miloc[idx_valid], y\u001b[38;5;241m.\u001b[39miloc[idx_valid]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 8,  \n",
    "    \"learning_rate\": 0.01,\n",
    "    \"n_estimators\": 10000,  \n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"verbose\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 0.3,\n",
    "    \"reg_lambda\": 8,\n",
    "    \"extra_trees\":True,\n",
    "    'num_leaves':32,\n",
    "    \"sample_weight\":'balanced',\n",
    "    \"device\": \"cpu\", \n",
    "    # \"device\": \"gpu\", \n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "fitted_models = []\n",
    "cv_scores = []\n",
    "\n",
    "best_auc = 0\n",
    "best_model = None\n",
    "\n",
    "for idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#   Because it takes a long time to divide the data set, \n",
    "    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# each time the data set is divided, two models are trained to each other twice, which saves time.\n",
    "    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set = [(X_valid, y_valid)],\n",
    "        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(100)] )\n",
    "    fitted_models.append(model)\n",
    "    y_pred_valid = model.predict_proba(X_valid)[:,1]\n",
    "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "    cv_scores.append(auc_score)\n",
    "    \n",
    "    if auc_score > best_auc:\n",
    "        best_auc = auc_score\n",
    "        best_model = model\n",
    "\n",
    "if best_model is not None:\n",
    "    best_model.booster_.save_model('LGBMClassifier_best_model.txt')\n",
    "\n",
    "print(\"CV AUC scores: \", cv_scores)\n",
    "print(\"Maximum CV AUC score: \", max(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd96958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
